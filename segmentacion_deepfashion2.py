# -*- coding: utf-8 -*-
"""segmentacion_deepfashion2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15MhjbI01QOgGSnkAu-CfEOz5TQ-H_PqT

#Segmentacion DeepFashion2 Custom COCO dataset (Detectron2)

Detectron2 es un framework de detección y segmentación potente creado por el centro de investigación de Facebook AI.

Se utiliza PyTorch (compatibles con las últimas versiones), para más info sobre entrenamiento rápido consultar (https://detectron2.readthedocs.io/notes/benchmarks.html).

Intro post: https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/

El real poder de Detectron2 se basa en la transferencia de aprendizaje con modelos pre-entrenados, disponibles en (https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md)

Lo único que resta es hacer "fine-tuning" sobre tu propio dataset.

#Instalación.

Primero se instalan las dependencias.
"""

!pip install -U 'git+https://github.com/facebookresearch/detectron2.git'
# install dependencies: (use cu101 because colab has CUDA 10.1)
!pip install -U torch==1.5 torchvision==0.6 -f https://download.pytorch.org/whl/cu101/torch_stable.html 
!pip install cython pyyaml==5.1
!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'
import torch, torchvision
print(torch.__version__, torch.cuda.is_available())
!gcc --version
# opencv is pre-installed on colab
!pip install detectron2==0.1.3 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html

"""#Descarga del Dataset

Como el ambiente colab no es suficiente para entrenar se utilizará el set de validación.
"""

# !mkdir /content/DeepFashion2/
# !gdown --id 1TIrJR8mMhXEWqPZByXo999_lRq455Izz -O /content/DeepFashion2/train.zip
!mkdir /content/DeepFashion2/
!gdown --id 1-e6J3rm1hXITaLQRtKSzgMd5bwm0J6wu -O /content/DeepFashion2/validation.zip

# !unzip -P "2019Deepfashion2**" /content/DeepFashion2/train.zip -d /content/DeepFashion2/
!unzip -P "2019Deepfashion2**" /content/DeepFashion2/validation.zip -d /content/DeepFashion2/
import os
os.remove("/content/DeepFashion2/validation.zip")

# !ls /content/DeepFashion2/train/image | wc -l
# !ls /content/DeepFashion2/train/annos | wc -l

!ls /content/DeepFashion2/validation/image | wc -l
!ls /content/DeepFashion2/validation/annos | wc -l

"""#Converting to COCO format

DeepFashion2 no viene en formato COCO, se debe transformar a formato coco.Puede encontrarse el dataset acá.(https://github.com/switchablenorms/DeepFashion2/blob/master/evaluation/deepfashion2_to_coco.py).
"""

#Nombres labels  en ingles Deepfashion2
# lst_name = ['short_sleeved_shirt', 'long_sleeved_shirt',
#             'short_sleeved_outwear', 'long_sleeved_outwear',
#             'vest', 'sling', 'shorts', 'trousers', 'skirt',
#             'short_sleeved_dress',
#             'long_sleeved_dress', 'vest_dress', 'sling_dress']

#Nombres labels en español Deepfashion2
lst_name = ['polera-camisa_manga_corta', 'polera-camisa_manga_larga',
            'abrigo_corto', 'abrigo_largo',
            'chaleco', 'polera_pabilo', 'short', 'pantalones', 'falda',
            'vestido_manga_corta',
            'vestido_manga_larga', 'vestido_sin_manga', 'solera']

from PIL import Image
import numpy as np
import json


dataset = {
    "info": {},
    "licenses": [],
    "images": [],
    "annotations": [],
    "categories": []
}


for idx, e  in enumerate(lst_name):
    dataset['categories'].append({
        'id': idx + 1,
        'name': e,
        'supercategory': "clothes",
        'keypoints': ['%i' % (i) for i in range(1, 295)],
        'skeleton': []
    })

num_images = 32153 #191961 
sub_index = 0  # the index of ground truth instance
for num in range(1, num_images + 1):
    json_name = '/content/DeepFashion2/validation/annos/' + str(num).zfill(6) + '.json'
    image_name = '/content/DeepFashion2/validation/image/' + str(num).zfill(6) + '.jpg'

    if (num >= 0):
        imag = Image.open(image_name)
        width, height = imag.size
        with open(json_name, 'r') as f:
            temp = json.loads(f.read())
            pair_id = temp['pair_id']

            dataset['images'].append({
                'coco_url': '',
                'date_captured': '',
                'file_name': str(num).zfill(6) + '.jpg',
                'flickr_url': '',
                'id': num,
                'license': 0,
                'width': width,
                'height': height
            })
            for i in temp:
                if i == 'source' or i == 'pair_id':
                    continue
                else:
                    points = np.zeros(294 * 3)
                    sub_index = sub_index + 1
                    box = temp[i]['bounding_box']
                    w = box[2] - box[0]
                    h = box[3] - box[1]
                    x_1 = box[0]
                    y_1 = box[1]
                    bbox = [x_1, y_1, w, h]
                    cat = temp[i]['category_id']
                    style = temp[i]['style']
                    seg = temp[i]['segmentation']
                    landmarks = temp[i]['landmarks']

                    points_x = landmarks[0::3]
                    points_y = landmarks[1::3]
                    points_v = landmarks[2::3]
                    points_x = np.array(points_x)
                    points_y = np.array(points_y)
                    points_v = np.array(points_v)
                    case = [0, 25, 58, 89, 128, 143, 158, 168,
                            182, 190, 219, 256, 275, 294]
                    idx_i, idx_j = case[cat - 1], case[cat]

                    for n in range(idx_i, idx_j):
                        points[3 * n] = points_x[n - idx_i]
                        points[3 * n + 1] = points_y[n - idx_i]
                        points[3 * n + 2] = points_v[n - idx_i]

                    num_points = len(np.where(points_v > 0)[0])

                    dataset['annotations'].append({
                        'area': w * h,
                        'bbox': bbox,
                        'category_id': cat,
                        'id': sub_index,
                        'pair_id': pair_id,
                        'image_id': num,
                        'iscrowd': 0,
                        'style': style,
                        'num_keypoints': num_points,
                        'keypoints': points.tolist(),
                        'segmentation': seg,
                    })

json_name = '/content/DeepFashion2/deepfashion2_validation.json'
with open(json_name, 'w') as f:
    json.dump(dataset, f)

dataset['annotations'][0].get('bbox')

"""#Imports"""

# Setup detectron2 logger
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# import some common libraries
import os
import numpy as np
import pandas as pd
import cv2
import random
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt
from matplotlib.ticker import AutoMinorLocator
from matplotlib import gridspec
import copy

# import some common detectron2 utilities
import detectron2
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.engine import DefaultTrainer
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
import torch
import torchvision


from detectron2.data import DatasetMapper
from detectron2.data import DatasetCatalog, MetadataCatalog
from detectron2.data import build_detection_test_loader
from detectron2.data import build_detection_train_loader
from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.structures import BoxMode
import detectron2.data.transforms as T
from detectron2.data import detection_utils as utils

"""#Análisis exploratorio del dataset"""

from pycocotools.coco import COCO
#train_annot_path = '/content/DeepFashion2/deepfashion2_train.json'
val_annot_path = '/content/DeepFashion2/deepfashion2_validation.json'
#train_coco = COCO(train_annot_path) # load annotations for training set
val_coco = COCO(val_annot_path) # load annotations for validation set


# function iterates ofver all ocurrences of a
#  person and returns relevant data row by row
def get_meta(coco):
    ids = list(coco.imgs.keys())
    for i, img_id in enumerate(ids):
        img_meta = coco.imgs[img_id]
        ann_ids = coco.getAnnIds(imgIds=img_id)
        # basic parameters of an image
        img_file_name = img_meta['file_name']
        w = img_meta['width']
        h = img_meta['height']
        # retrieve metadata for all persons in the current image
        anns = coco.loadAnns(ann_ids)
        yield [img_id, img_file_name, w, h, anns]

# iterate over images
for img_id, img_fname, w, h, meta in get_meta(val_coco):
    # iterate over all annotations of an image
    for m in meta:
        # m is a dictionary
        keypoints = m['keypoints']

def convert_to_df(coco):
    images_data = []
    style_data = []
    test_data = []
    # iterate over all images
    for img_id, img_fname, w, h, meta in get_meta(coco):
        images_data.append({
            'image_id': int(img_id),
            'path': img_fname,
            'width': int(w),
            'height': int(h)
        })
        #iterate over all metadata
        for m in meta:
          test_data.append(m)
          style_data.append({
                'image_id': m['image_id'],
                'bbox': m['bbox'],
                'area': m['area'],
                'category_id': m['category_id'],
                'id': m['id'],
                'pair_id': m['pair_id'],
                'iscrowd': m['iscrowd'],
                'style': m['style'],
                'num_keypoints': m['num_keypoints'],
                'keypoints': m['keypoints']
            })
    # create dataframe with image paths
    images_df = pd.DataFrame(images_data)
    images_df.set_index('image_id', inplace=True)
    t = pd.DataFrame(test_data)
    t.set_index('image_id', inplace=True)
    # create dataframe with style
    style_df = pd.DataFrame(style_data)
    style_df.set_index('image_id', inplace=True)
    return images_df, style_df,t

# images_df, persons_df = convert_to_df(val_coco)
# train_coco_df = pd.merge(images_df, persons_df, right_index=True, left_index=True)
# train_coco_df['source'] = 0

# images_df, persons_df = convert_to_df(val_coco)
images_df, styles_df, t = convert_to_df(val_coco)
val_coco_df = pd.merge(images_df, styles_df, right_index=True, left_index=True)
val_coco_df['source'] = 1
#coco_df = pd.concat([train_coco_df, val_coco_df], ignore_index=True)
coco_df = pd.concat([val_coco_df], ignore_index=True)

#Función para extraer la cantidad de prendas presentes en la base con formato
#COCO
def countn(lst,cloth_names=lst_name):
  tuple_cloth_counts = []
  for i in range(1,13):
    to_append = (cloth_names[i-1],lst.count(i))
    tuple_cloth_counts.append(to_append)
  return dict(tuple_cloth_counts)

#Se aplica la función y se crea un dataframe para el plot
cloth_counts = countn(t.category_id.values.tolist())
df_cloth_counts = pd.DataFrame.from_dict(cloth_counts,orient='index',
                                         columns=['cantidad'])
df_cloth_counts = df_cloth_counts.reset_index()
df_cloth_counts.columns = ['tipo_ropa','cantidad']
df_cloth_counts_sorted = df_cloth_counts.sort_values('cantidad')
df_cloth_counts_sorted

plt.figure(figsize=(16,9))

df_cloth_counts_sorted = df_cloth_counts.sort_values('cantidad')

plt.bar('tipo_ropa', 'cantidad', data=df_cloth_counts_sorted)
plt.xticks(rotation='vertical')
plt.xlabel('Tipo de atuendo', size=15)
plt.title('Cantidad de prendas de vestuario presentes en la base',
          loc = 'left', fontsize = 18)
plt.grid()
plt.show()

val_coco_df

"""#Registrar Dataset"""

from detectron2.data.datasets import register_coco_instances
#register_coco_instances("deepfashion_train", {},
# "/content/DeepFashion2/deepfashion2_train.json",
# "/content/DeepFashion2/train/image")
register_coco_instances("deepfashion_val", {},
                        "/content/DeepFashion2/deepfashion2_validation.json",
                        "/content/DeepFashion2/validation/image")

"""#Config para Entrenamiento

Necesitamos crear el objeto config y settear hiperparámetros adecuados.

Acá se utilizará Faster R-CNN backbone, puede encontrarse otro detectron2 pre-entrenado en el model-zoo. (https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md).
"""

from detectron2.data import detection_utils as utils
import detectron2.data.transforms as T
import copy

def custom_mapper(dataset_dict):
    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below
    image = utils.read_image(dataset_dict["file_name"], format="BGR")
    transform_list = [# T.Resize((800,600)),
        T.RandomBrightness(0.8, 1.8),
        T.RandomContrast(0.6, 1.3),
        T.RandomSaturation(0.8, 1.4),
        #T.RandomRotation(angle=[90, 90]),
        T.RandomLighting(0.7),
        #T.RandomFlip(prob=0.5, horizontal=False, vertical=True),
    ]
    image, transforms = T.apply_transform_gens(transform_list, image)
    dataset_dict["image"] = torch.as_tensor(image.transpose(2, 0, 1).astype("float32"))

    annos = [
        utils.transform_instance_annotations(obj, transforms, image.shape[:2])
        for obj in dataset_dict.pop("annotations")
        if obj.get("iscrowd", 0) == 0
    ]
    instances = utils.annotations_to_instances(annos, image.shape[:2])
    dataset_dict["instances"] = utils.filter_empty_instances(instances)
    return dataset_dict

from detectron2.engine import DefaultTrainer
from detectron2.data import build_detection_test_loader, build_detection_train_loader

class CustomTrainer(DefaultTrainer):
    @classmethod
    def build_train_loader(cls, cfg):
        return build_detection_train_loader(cfg, mapper=custom_mapper)

from detectron2.data import DatasetMapper   # the default mapper
cfg = get_cfg()
#cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml"))
cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("deepfashion_val",)
cfg.DATASETS.TEST = ()
#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml") 
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")

cfg.DATALOADER.NUM_WORKERS = 4
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.00025
cfg.SOLVER.WARMUP_ITERS = 1000
cfg.SOLVER.MAX_ITER = 2000
cfg.SOLVER.STEPS = (1000, 1500)
cfg.SOLVER.GAMMA = 0.05
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 13

cfg.TEST.EVAL_PERIOD = 500

"""# Entrenamiento

Se comienza a entrenar después de ajustar los hiperparámetros
"""

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
# trainer = DefaultTrainer(cfg)
trainer = CustomTrainer(cfg)
trainer.resume_or_load(resume=False)
trainer.train()

# Commented out IPython magic to ensure Python compatibility.
# Look at training curves in tensorboard:
# %load_ext tensorboard
#%reload_ext tensorboard
# %tensorboard --logdir output

"""# Predict

Para crear el objeto predictor necesitamos settear algunos parámetros, el umbral se settea bastante bajo debido a que se entrena con validación y las iteraciones son bajas.
"""

cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.55  # set the testing threshold for this model
cfg.DATASETS.TEST = ("deepfashion_val", )
predictor = DefaultPredictor(cfg)

"""Test del modelo entrenado"""

!wget https://img-lcwaikiki.mncdn.com/mnresize/1024/-//productimages/20201/1/3945185/l_20201-0sg016z8-cs8_a.jpg -O /content/example.jpg

from detectron2.utils.visualizer import ColorMode
im = cv2.imread("/content/2000377884300_2.jpg")
outputs = predictor(im)

"""Visualizaciones"""

# We can use `Visualizer` to draw the predictions on the image.
v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
cv2_imshow(v.get_image()[:, :, ::-1])

from detectron2.utils.visualizer import ColorMode
im = cv2.imread("/content/2000378136224_2.jpg")
outputs = predictor(im)

# We can use `Visualizer` to draw the predictions on the image.
v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
cv2_imshow(v.get_image()[:, :, ::-1])

from detectron2.utils.visualizer import ColorMode
im = cv2.imread("/content/2000379182459_2.jpg")
outputs = predictor(im)

# We can use `Visualizer` to draw the predictions on the image.
v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
cv2_imshow(v.get_image()[:, :, ::-1])

from detectron2.utils.visualizer import ColorMode
im = cv2.imread("/content/2000379210398-1.jpg")
outputs = predictor(im)

# We can use `Visualizer` to draw the predictions on the image.
v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
cv2_imshow(v.get_image()[:, :, ::-1])

from detectron2.utils.visualizer import ColorMode
im = cv2.imread("/content/2000379295548-1.jpg")
outputs = predictor(im)

# We can use `Visualizer` to draw the predictions on the image.
v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
cv2_imshow(v.get_image()[:, :, ::-1])

from detectron2.utils.visualizer import ColorMode
im = cv2.imread("2000379303427-1.jpg")
outputs = predictor(im)

# We can use `Visualizer` to draw the predictions on the image.
v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
cv2_imshow(v.get_image()[:, :, ::-1])

